{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ShakespeareBot RNN_done.ipynb","provenance":[{"file_id":"1gKudLppELjlkHraxfRqUfQRWULCtFrWZ","timestamp":1615106078489}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2d0IENetcVKk"},"source":["import os\n","import re\n","import random\n","import urllib.request\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import time\n","\n","shakespeare = urllib.request.urlopen('https://raw.githubusercontent.com/lakigigar/Caltech-CS155-2021/main/projects/project3/data/shakespeare.txt').read().decode('utf-8')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZSas4Qt7r2Lt"},"source":["# LSTM Model Data Processing and Poem Generation\n"]},{"cell_type":"markdown","metadata":{"id":"CbbfCwD_xcr-"},"source":["Break training data into 40-character sequences"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"NVrfKjnisBfl","executionInfo":{"status":"ok","timestamp":1616999484225,"user_tz":420,"elapsed":389,"user":{"displayName":"Tony Liu","photoUrl":"","userId":"01450533415788947646"}},"outputId":"781562e3-afa6-4089-da6d-e9bfac704fbe"},"source":["# LSTM Model Definition \n","# Based on Jason Brownle's approach \n","\n","window_size = 40\n","skip_len = 1\n","\n","# Split training data into len_seqs-length character sequences\n","def build_char_seqs(data, n_freq, len_seqs=40):\n","# len_seqs is length of training sequences, starting at every n_freq character\n","  \n","  # Put all text in single line\n","  # Each line is separated by \"\\n\"\n","  # Each poem is separted by \" \\n\"\n","  # Normalize to lowercase letters\n","  \n","  data = re.sub('\\r\\n\\r\\n\\r\\n ','',data)\n","  tokens = re.findall(r'\\S+|\\n',data)\n","  data = ' '.join(tokens).lower()\n","  raw_text = data\n","  # Remove all numbers\n","  training_data = ''.join([i for i in raw_text if not i.isdigit()])\n","  # Remove all punctuations\n","  training_data = training_data.replace(',', '')\n","  training_data = training_data.replace('.', '')\n","  training_data = training_data.replace('?', '')\n","  training_data = training_data.replace('!', '')\n","  training_data = training_data.replace(':', '')\n","  training_data = training_data.replace(';', '')\n","  training_data = training_data.replace('(', '')\n","  training_data = training_data.replace(')', '')\n","\n","  sequences = []\n","  \n","  # Create len_seqs-length character sequences\n","  for i in range(len_seqs, len(training_data), n_freq):\n","  \n","    sequences.append(training_data[i-len_seqs:i+1])\n","\n","  return training_data, sequences\n","\n","processed_text, sequences = build_char_seqs(shakespeare,n_freq=skip_len, len_seqs=window_size)\n","\n","print('Total Sequences: %d' % len(sequences))\n","print(sequences[0:2])\n","print(processed_text[0:500])\n","processed_text[0:500]\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Sequences: 95429\n","[' \\n from fairest creatures we desire incre', '\\n from fairest creatures we desire increa']\n"," \n"," from fairest creatures we desire increase \n"," that thereby beauty's rose might never die \n"," but as the riper should by time decease \n"," his tender heir might bear his memory \n"," but thou contracted to thine own bright eyes \n"," feed'st thy light's flame with self-substantial fuel \n"," making a famine where abundance lies \n"," thy self thy foe to thy sweet self too cruel \n"," thou that art now the world's fresh ornament \n"," and only herald to the gaudy spring \n"," within thine own bud buriest thy content \n"," and tender\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" \\n from fairest creatures we desire increase \\n that thereby beauty's rose might never die \\n but as the riper should by time decease \\n his tender heir might bear his memory \\n but thou contracted to thine own bright eyes \\n feed'st thy light's flame with self-substantial fuel \\n making a famine where abundance lies \\n thy self thy foe to thy sweet self too cruel \\n thou that art now the world's fresh ornament \\n and only herald to the gaudy spring \\n within thine own bud buriest thy content \\n and tender\""]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"F3nX5TuAxlwn"},"source":["Encode character sequences as integers -> one hot, break into training input and labels"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbIBPsrwxMiz","executionInfo":{"status":"ok","timestamp":1616999496846,"user_tz":420,"elapsed":2911,"user":{"displayName":"Tony Liu","photoUrl":"","userId":"01450533415788947646"}},"outputId":"e9bc865f-6152-4112-f677-438a1a0dc432"},"source":["lines = sequences\n","\n","# Create mapping dictionary to encode characters as integers\n","chars = sorted(list(set(processed_text)))\n","mapping = dict((c, i) for i, c in enumerate(chars))\n","\n","# For each 40-character input, convert it to integers from dict\n","encoded_seqs = []\n","for line in lines:\n"," encoded_seqs.append([mapping[char] for char in line])\n","\n","# Get vocabulary size\n","vocab_size = len(mapping)\n","print('Vocabulary Size: %d' % vocab_size)\n","\n","\n","# Create input and output, where output is single next character in seq\n","encoded_seqs = np.array(encoded_seqs)\n","X, y = encoded_seqs[:,:-1], encoded_seqs[:,-1]\n","\n","\n","# One-hot encode the integer sequences\n","int_sequences = [tf.keras.utils.to_categorical(x, num_classes=vocab_size) for x in X]\n","X = np.array(int_sequences)\n","y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n","\n","print(mapping)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 30\n","{'\\n': 0, ' ': 1, \"'\": 2, '-': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u575qgf4x3Xo"},"source":["#Build and fit LSTM model\n","##higher temperature -> softer destribution -> less confident -> diverse generation\n","##lower temperature -> sharper destribution -> more confident -> repetitive generation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OK7XwbNkm94p","executionInfo":{"status":"ok","timestamp":1616999523935,"user_tz":420,"elapsed":316,"user":{"displayName":"Tony Liu","photoUrl":"","userId":"01450533415788947646"}},"outputId":"343a10c1-2853-4efa-c35b-cb2468810ae9"},"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","\n","tf.config.list_physical_devices('GPU')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num GPUs Available:  1\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-45WJEJox3nX","executionInfo":{"status":"ok","timestamp":1617000302916,"user_tz":420,"elapsed":771018,"user":{"displayName":"Tony Liu","photoUrl":"","userId":"01450533415788947646"}},"outputId":"73e43b25-452d-40fd-e0d3-94a58699c8c0"},"source":["start_time = time.time()\n","\n","temperature = 0.25\n","\n","# Define LSTM Model\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n","\n","# Use a lambda layer to scale the output array by temperature\n","model.add(tf.keras.layers.Lambda(lambda x: x / temperature))\n","model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n","print(model.summary())\n","\n","# Compile model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Fit model\n","model.fit(X, y, epochs=50)\n","\n","print(\"--- %s seconds used ---\" % (time.time() - start_time))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_5 (LSTM)                (None, 200)               184800    \n","_________________________________________________________________\n","lambda_5 (Lambda)            (None, 200)               0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 30)                6030      \n","=================================================================\n","Total params: 190,830\n","Trainable params: 190,830\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/50\n","2983/2983 [==============================] - 17s 5ms/step - loss: 2.2770 - accuracy: 0.3434\n","Epoch 2/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.7266 - accuracy: 0.4804\n","Epoch 3/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.5815 - accuracy: 0.5140\n","Epoch 4/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.4935 - accuracy: 0.5397\n","Epoch 5/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.4100 - accuracy: 0.5599\n","Epoch 6/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.3499 - accuracy: 0.5769\n","Epoch 7/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.2825 - accuracy: 0.5958\n","Epoch 8/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.2325 - accuracy: 0.6073\n","Epoch 9/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.1713 - accuracy: 0.6268\n","Epoch 10/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.1138 - accuracy: 0.6445\n","Epoch 11/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 1.0492 - accuracy: 0.6624\n","Epoch 12/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.9951 - accuracy: 0.6767\n","Epoch 13/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.9424 - accuracy: 0.6969\n","Epoch 14/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.8901 - accuracy: 0.7130\n","Epoch 15/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.8369 - accuracy: 0.7309\n","Epoch 16/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.7901 - accuracy: 0.7444\n","Epoch 17/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.7551 - accuracy: 0.7557\n","Epoch 18/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.7076 - accuracy: 0.7711\n","Epoch 19/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.6716 - accuracy: 0.7860\n","Epoch 20/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.6366 - accuracy: 0.7942\n","Epoch 21/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.6144 - accuracy: 0.8005\n","Epoch 22/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.5822 - accuracy: 0.8111\n","Epoch 23/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.5505 - accuracy: 0.8219\n","Epoch 24/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.5289 - accuracy: 0.8296\n","Epoch 25/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.5182 - accuracy: 0.8329\n","Epoch 26/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.4884 - accuracy: 0.8421\n","Epoch 27/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.4700 - accuracy: 0.8473\n","Epoch 28/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.4526 - accuracy: 0.8512\n","Epoch 29/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.4398 - accuracy: 0.8557\n","Epoch 30/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.4195 - accuracy: 0.8619\n","Epoch 31/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.4091 - accuracy: 0.8674\n","Epoch 32/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.3996 - accuracy: 0.8687\n","Epoch 33/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.3814 - accuracy: 0.8761\n","Epoch 34/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.3743 - accuracy: 0.8765\n","Epoch 35/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.3630 - accuracy: 0.8811\n","Epoch 36/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.3466 - accuracy: 0.8862\n","Epoch 37/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.3406 - accuracy: 0.8885\n","Epoch 38/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.3325 - accuracy: 0.8905\n","Epoch 39/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.3236 - accuracy: 0.8924\n","Epoch 40/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.3110 - accuracy: 0.8976\n","Epoch 41/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.3060 - accuracy: 0.8982\n","Epoch 42/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.3068 - accuracy: 0.8988\n","Epoch 43/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.2927 - accuracy: 0.9032\n","Epoch 44/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.2851 - accuracy: 0.9049\n","Epoch 45/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.2830 - accuracy: 0.9065\n","Epoch 46/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.2785 - accuracy: 0.9079\n","Epoch 47/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.2748 - accuracy: 0.9087\n","Epoch 48/50\n","2983/2983 [==============================] - 16s 5ms/step - loss: 0.2656 - accuracy: 0.9122\n","Epoch 49/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.2621 - accuracy: 0.9133\n","Epoch 50/50\n","2983/2983 [==============================] - 15s 5ms/step - loss: 0.2577 - accuracy: 0.9162\n","--- 770.7565739154816 seconds used ---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a8sdO2Fdx9m0"},"source":["# Poem Generation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QlvD-WUx_0m","executionInfo":{"status":"ok","timestamp":1617000336229,"user_tz":420,"elapsed":285781,"user":{"displayName":"Tony Liu","photoUrl":"","userId":"01450533415788947646"}},"outputId":"534d8de3-7909-4c71-d923-40aff0ac149f"},"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","# generate a sequence of characters with a language model\n","def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n","    in_text = seed_text\n","\n","    # generate a fixed number of characters\n","    for _ in range(n_chars):\n","        # encode the characters as integers\n","        encoded = [mapping[char] for char in in_text]\n","\n","        # truncate sequences to a fixed length\n","        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","\n","        # one hot encode\n","        encoded = to_categorical(encoded, num_classes=len(mapping))\n","        # predict character, output the one with highest probability\n","        yhat = model.predict_classes(encoded, verbose=0)\n","        # reverse map integer to character\n","        out_char = ''\n","        for char, index in mapping.items():\n","            if index == yhat:\n","                out_char = char\n","                break\n","        # append to input\n","        in_text += char\n","    return in_text\n","\n","\n","# Try seeding with the first 40 characters of training data (seed can be any string)\n","seed = \"shall i compare thee to a summer's day\" \n","raw_poem = generate_seq(model, mapping, window_size, seed, 1000)\n","print(raw_poem)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["shall i compare thee to a summer's day \n"," thou bring for thou art force what were not see dies \n"," by thy dead fleech and despite thy self decease \n"," her it alteren that thy soul that thou \n"," rocks are in this his give men's eyes can lend \n"," that be to thee and common place sorit \n"," could do womadsing and is posesson state \n"," and in the stormy gusts of memory \n"," my music handow and there beseem every the winds \n"," so all my argument tall not praise that tong \n"," hape your seaker shape is my still to say \n"," to thee and common place should prove thie  \n"," since say mine we abundatiplion in \n"," on your trosph into this praise the worst \n"," and therefore of their styll 's sin approud \n"," that my staile that merich and every show \n"," the mation caul being fulst of graces part \n"," and salve my love's spite i wit you awand \n"," or thou art conterpesate \n"," which in thy couts day so pier in spill \n"," to see him those bluch and sweetless absence \n"," have that i compise to thy constancy \n"," so thou be rich nor recopsy with thy stay \n"," for eterning with taulthe tobe thee\n"],"name":"stdout"}]}]}